{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "c:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "#import dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "from rich import print\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import yake\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (run this only once)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CUDA Available: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CUDA Available: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device Name: NVIDIA GeForce RTX <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3050</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Device Name: NVIDIA GeForce RTX \u001b[1;36m3050\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activando CUDA para acelerar el procesamiento de vectores ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_name = 'TestC'\n",
    "LLM_model = 'mistral'\n",
    "temperature_parameter=0\n",
    "retriever_matches_k = 3\n",
    "\n",
    "## Save each iteration to file\n",
    "output_file_path = '../Predicciones_Mistral_TestC.csv'\n",
    "#output_file_path = '../Predicciones_GPT3_5Turbo0125_OpenAI_table.csv'\n",
    "\n",
    "df = pd.read_csv('../validation_dataset/Validation data reviewed.csv', encoding='utf-8', dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir =  os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir,'..' ,'index', index_test_name))\n",
    "\n",
    "embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#embed_model = \"text-embedding-3-large\"\n",
    "\n",
    "# Para correr las predicciones en GPU\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea la estructura del RAG\n",
    "Utiliza HuggingFaceInstructEmbeddings para generar embeddings de descripciones de productos.\n",
    "\n",
    "Almacena y gestiona embeddings en una base de datos vectorial Chroma.\n",
    "\n",
    "Recupera documentos más relevantes basados en similitud con un retriever ajustado a los mejores 3 resultados.\n",
    "\n",
    "Utiliza el modelo de lenguaje Llama3 para predecir el código HS más preciso a partir de descripciones.\n",
    "\n",
    "Limita la respuesta a un único código HS, extraído del documento con mayor similitud.\n",
    "\n",
    "Define una plantilla personalizada para guiar las respuestas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "##### Embedding model (sentence-transformer)\n",
    "model_name = embed_model\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "##### LLM model\n",
    "# loading the Llama3 model from local\n",
    "llm_model = OllamaLLM(model=LLM_model,\n",
    "                temperature=temperature_parameter,\n",
    "                num_thread=8,\n",
    "                )\n",
    "# loading the vectorstore\n",
    "vectorstore = Chroma(persist_directory=persistent_dir, embedding_function=hf_embed_model)\n",
    "# casting  the vectorstore as the retriever, taking the best 3 similarities\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":retriever_matches_k})\n",
    "\n",
    "# formating the docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#template = \"\"\"You must respond only with the hs_code from the metadata of the source_documents retrieved by the retriever.\n",
    "#with the highest similarity score, avoid responding with any kind of text \n",
    "#even if you can be sure if the HS code is correct like \n",
    "#\"I'm unable to provide a response...\" or \"I cannot provide a response\".\n",
    "\n",
    "\n",
    "template = \"\"\"You must strictly respond with the HS code that matches the information contained in the 'hs_code' \n",
    "field of the metadata from the source_documents provided. You are not allowed to respond with any HS code from your own knowledge. \n",
    "Do not invent or guess. Only respond with one HS code from the provided metadata that has the highest relevance score from the retriever, \n",
    "and it must be from the 'hs_code' field.\n",
    "\n",
    "For example, if the source document mentions 'Potato starch' with 'hs_code': '1108.13', your answer must be '1108.13'.\n",
    "\n",
    "Avoid responding with any other text.Respond only with 1 HS code, the one with better score from the retriever.\n",
    "\n",
    "context:\n",
    "{summaries}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Define the LLM chain (using the Llama3.1 model)\n",
    "llm_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm_model,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # To get both the answer and source docs\n",
    "    chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                #For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "                input_variables=[\"question\", \"summaries\"],\n",
    "            ),\n",
    "        },\n",
    ")\n",
    "\n",
    "context = \"\"\"As a logistics shipping arrival inspector, your primary responsibility is to inspect incoming shipments and accurately classify goods \n",
    "using the Harmonized System (HS) code based on the descriptions provided in the shipping manifests. You will thoroughly review the manifest details, \n",
    "including product type, material composition, function, and intended use, to determine the correct HS code. \n",
    "\n",
    "Your task is to:\n",
    "Carefully read and analyze the product descriptions from the manifest.\n",
    "Identify key characteristics of the goods, such as \n",
    "type (e.g., electronics, textiles, machinery), \n",
    "material (e.g., plastic, metal, organic), \n",
    "and usage (e.g., household, industrial, medical).\n",
    "Use your knowledge of the HS code classification system to assign the most appropriate HS code for each product based on its description.\n",
    "Ensure compliance with international trade regulations by selecting precise codes to avoid delays or penalties.\n",
    "Remember to be thorough and accurate in your classification, as this impacts customs processing, tariffs, and legal requirements.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up YAKE keyword extractor\n",
    "yake_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.1\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "\n",
    "# Load the translation model (use Spanish-to-English model)\n",
    "model_name = 'Helsinki-NLP/opus-mt-es-en'  # Spanish to English model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Get the English words corpus and English stopwords\n",
    "#english_words = set(words.words())\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "# Add your custom stopwords\n",
    "additional_stopwords = {\n",
    "    # Add your custom stopwords here (same as your provided list)\n",
    "    'hs', 'code', 'hscode', 'hs-code', 'hs  code', 'pallets', 'plts', 'shipper', 'declares', 'hs code',\n",
    "    'containing', 'contains', 'meter', 'cubic', 'packages', 'load', 'loaded', 'weight', \n",
    "    'netweight', 'kg', 'kgs', 'cb', 'cbm', 'goods', 'parts', 'pieces', 'accessories', 'packing', \n",
    "    'declared', 'dangerous', 'impression', 'items', 'sheets', 'codes', \n",
    "    'sin', 'impresion', 'containers', 'pc', 'abv', 'net', 'gross', 'cif', 'aduana', 'customs', \n",
    "    'value', 'tax', 'duty', 'freight', 'port', 'terminal', 'consignee', 'consignor', 'invoice', \n",
    "    'manifest', 'quantity', 'description', 'volume', 'packaging', 'shipment', 'delivery', 'origin', \n",
    "    'destination', 'transport', 'carrier', 'export', 'import', 'tariff', 'item', 'declaration', \n",
    "    'clearance', 'documentation', 'commercial', 'charge', 'fees', 'logistics', 'shipping', \n",
    "    'container', 'unit', 'measurement', 'certification', 'palletized', 'metric', 'commodity', \n",
    "    'classification', 'entry', 'exportation', 'importation', 'bonded', 'zone', 'trade', 'license', 'bottle', 'bottles', 'cl',\n",
    "    'ancho', 'largo', 'mm', 'pcs', 'xhc', 'stc', 'uks','x','k', 'pty', 'id', 'cp', 'ncm', 'ne', 'itpa', 'zz', 'xg', 'topmag',\n",
    "    'rtmx', 'fcl', 'cf','f', 'xdc', 'pkgs', 'voice', 'n', 'per', 'email'\n",
    "}\n",
    "\n",
    "def lemmatize_translate_clean(text):\n",
    "    keywords = text.split()\n",
    "    valid_keywords = [keyword for keyword in keywords if keyword not in additional_stopwords]\n",
    "    lematized_keywords = [lemmatizer.lemmatize(keyword) for keyword in valid_keywords]\n",
    "\n",
    "    ### Lemmatize the extracted keywords\n",
    "    ##lemmatized_keywords = [lemmatizer.lemmatize(keyword[0]) for keyword in keywords]\n",
    "    lemmatized_sentence = \" \".join(lematized_keywords)\n",
    "\n",
    "    ### Correct typos in the lemmatized keywords\n",
    "    ##corrected_keywords = [str(TextBlob(keyword).correct()) for keyword in lemmatized_keywords]\n",
    "    ##lematized_and_corrected_typos_sentence = \" \".join(corrected_keywords)\n",
    "\n",
    "    ##lemmatize_translate_clean_text = []\n",
    "    ##flattened_items = [word for item in lemmatized_keywords for word in item.split()]\n",
    "    ###print(flattened_items)\n",
    "    ##for item in flattened_items:\n",
    "    ##    lemma = lemmatizer.lemmatize(item)\n",
    "    ##    #Removing condition to check if the word exists on english dictionary, cartulin was not there for example\n",
    "    ##    #if lemma in english_words and lemma not in english_stopwords:\n",
    "    ##    if lemma in lemma not in english_stopwords:            \n",
    "    ##        lemmatize_translate_clean_text.append(lemma)\n",
    "    #print(english_existing_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[\\d]+|[^\\w\\s]', '', text)  # Remove numbers and punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text, tokenizer, model):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Generate translation\n",
    "    translated = model.generate(**inputs)\n",
    "    # Decode and return the translated text\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Full pipeline function\n",
    "def process_text(text):\n",
    "    try:\n",
    "        # Step 1: Preprocess\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        \n",
    "        # Step 2: Translate\n",
    "        translated_text = translate_to_english(preprocessed_text, tokenizer, model)\n",
    "        \n",
    "        # Step 3: Lemmatize, remove nonsense words, and clean\n",
    "        final_cleaned_text = lemmatize_translate_clean(translated_text)\n",
    "\n",
    "        return final_cleaned_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de descripciones 'Hardcoded' para verificar viabilidad de corrida Montecarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">The requested item's description CLEANED to search HTS code is:</span>\n",
       "glassware ma rk mould noquantity\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mThe requested item's description CLEANED to search HTS code is:\u001b[0m\n",
       "glassware ma rk mould noquantity\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">The response of the LLM is:</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6905.20</span> <span style=\"font-weight: bold\">(</span>from the source document <span style=\"color: #008000; text-decoration-color: #008000\">\"Glassware, of lead crystal, of a kind used for\"</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mThe response of the LLM is:\u001b[0m\n",
       "\u001b[1;36m6905.20\u001b[0m \u001b[1m(\u001b[0mfrom the source document \u001b[32m\"Glassware, of lead crystal, of a kind used for\"\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The documents sent to the LLM are:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The documents sent to the LLM are:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">other carbons and ceramic or glass moulds<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "other carbons and ceramic or glass moulds\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'hs_code'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'8480.41'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'..\\\\data\\\\hs_code_dictionary.txt'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'hs_code'\u001b[0m: \u001b[32m'8480.41'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'..\\\\data\\\\hs_code_dictionary.txt'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Glassware, of lead crystal, of a kind used for\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Glassware, of lead crystal, of a kind used for\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'hs_code'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7013.91'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'..\\\\data\\\\hs_code_dictionary.txt'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'hs_code'\u001b[0m: \u001b[32m'7013.91'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'..\\\\data\\\\hs_code_dictionary.txt'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Glassware for table or kitchen purposes of glass\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Glassware for table or kitchen purposes of glass\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'hs_code'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7013.42'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'..\\\\data\\\\hs_code_dictionary.txt'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'hs_code'\u001b[0m: \u001b[32m'7013.42'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'..\\\\data\\\\hs_code_dictionary.txt'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#query = \"sodium lignosulphonate\"\n",
    "#query = \"kilogram organic coffee bean certified fairtrade utz roasted usa global distribution\"\n",
    "#query = \"prepaid cartons wine\"\n",
    "#query = \"wine\"\n",
    "#query = \"guide rod caliper\"\n",
    "#query = \"said contain plastic drum h msku dry shipper seal pw plastic drum h stowed methyl butyric acid corrosive liqu\"\n",
    "#query = \"modified potato starch starch emflo kp\"\n",
    "#query = \"CARTULINA NEGRA BOB 170 GRS HS CODE 480258 CARTULINA BLANCA BOB 210 GRS HS CODE 480258\"\n",
    "#query = \"wooden cases delivery list\"\n",
    "#query = \"pharmaceuticals aspirin protection code pal\"\n",
    "#query = \"plastic bottles\"\n",
    "#query = \"HERRAJES PARA MUEBLES HS CODE 83024200 EMAIL G.GIACOMO RAGO-GROUP.COM\"\n",
    "query = \"glassware ma rk mould noquantity\"\n",
    "\n",
    "# Clean the input text and send to LLM\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "result = llm_chain({\"question\": process_text(query), \"summaries\": context})\n",
    "print(f\"[bold yellow]The requested item's description CLEANED to search HTS code is:[/bold yellow]\\n{process_text(query)}\")\n",
    "print(f\"[bold green]The response of the LLM is:[/bold green]\\n{result['answer']}\")\n",
    "\n",
    "#print(result.keys())\n",
    "#print(result[\"summaries\"])\n",
    "#print(result[\"sources\"])\n",
    "\n",
    "print(\"The documents sent to the LLM are:\")\n",
    "for i in range(len(result[\"source_documents\"])):\n",
    "    print(result[\"source_documents\"][i].page_content)\n",
    "    print(result[\"source_documents\"][i].metadata)\n",
    "    \n",
    "#results = vectorstore.similarity_search_with_score(query=query, k=retriever_matches_k)\n",
    "#print(\"Top n coincidences from the index are:\")\n",
    "#for doc, score in results:\n",
    "#    print(f\"Document content: {doc.page_content}, Code: {doc.metadata},Similarity Score: {score}\")\n",
    "\n",
    "\n",
    "# Find the document with the highest score\n",
    "#max_doc, max_score = max(results, key=lambda x: x[1])\n",
    "#print(\"Best match from the index is:\")\n",
    "#print(f\"Document content: {max_doc.page_content}, Code: {max_doc.metadata}, Similarity Score: {max_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrida montecarlo, 50 iteraciones de muestreos de 30 despcripciones cada una. \n",
    "\n",
    "El resultado se vacia en un csv para verificacion detallada de cada codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accuracy achieved in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>% of the dataset: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.00</span>% <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-digit<span style=\"font-weight: bold\">)</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.00</span>% <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>-digit<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accuracy achieved in \u001b[1;36m100\u001b[0m% of the dataset: \u001b[1;36m20.00\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m-digit\u001b[1m)\u001b[0m, \u001b[1;36m5.00\u001b[0m% \u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m-digit\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_predictions(descriptions_to_predict):\n",
    "    \n",
    "    # Select 30 random queries\n",
    "    # Shuffle the original DataFrame\n",
    "    #df_shuffled = all_queries.sample(frac=1).reset_index(drop=True)\n",
    "    #selected_queries = df_shuffled.sample(descriptions_to_predict)\n",
    "\n",
    "    # Evaluate in order all the dataset\n",
    "    selected_queries = all_queries.head(descriptions_to_predict).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Now, process each query with llm_chain\n",
    "    #print(\"Predicting HS codes of random Validation Data rows...\")\n",
    "    for index, row in selected_queries.iterrows():\n",
    "        query = process_text(row['Raw_data_input'])\n",
    "        expected_output = row['Expected_output']\n",
    "\n",
    "        # Execute the LLM chain (using a mock prediction for illustration here)\n",
    "        result = llm_chain({\"question\": query, \"summaries\": context})  # This is the actual call\n",
    "        llm_prediction = result['answer']  # Assuming LLM returns a numeric 6-digit value as the prediction\n",
    "\n",
    "        # Append the result in the required structure\n",
    "        results.append({\n",
    "            \"Raw_data_input\": row['Raw_data_input'],\n",
    "            \"Raw_data_input_processed\": query,\n",
    "            \"LLM_prediction\": llm_prediction,  # Here, the prediction from LLM\n",
    "            \"Expected_output\": expected_output,\n",
    "            \"Expected_output_two_digits\": row['Expected_output_two_digits'],\n",
    "            \"Predicted_output_two_digits\": llm_prediction[:2],\n",
    "            \"CorrectMatch_two_digits\": 1 if llm_prediction[:2] == row['Expected_output_two_digits'] else 0 ,\n",
    "            \"Expected_output_four_digits\": row['Expected_output_four_digits'],\n",
    "            \"Predicted_output_four_digits\": llm_prediction[:4],\n",
    "            \"CorrectMatch_four_digits\": 1 if llm_prediction[:4] == row['Expected_output_four_digits'] else 0 \n",
    "        })\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    predictions_df = pd.DataFrame(results)\n",
    "\n",
    "    #Append iteration results to file\n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(output_file_path)\n",
    "\n",
    "    # Append to the file, write header only if file doesn't already exist\n",
    "    predictions_df.to_csv(output_file_path, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "# Function to calculate accuracy (both for 2 and 4 digits)\n",
    "def calculate_accuracy(iteration_df):\n",
    "    correct_two_digits = 0\n",
    "    correct_four_digits = 0\n",
    "    total = len(iteration_df)\n",
    "\n",
    "    for index, row in iteration_df.iterrows():\n",
    "        best_hs_code = str(row['LLM_prediction'])[:4]\n",
    "        hs_actual = str(row['Expected_output'])[:4]\n",
    "\n",
    "        # Check 2-digit accuracy\n",
    "        if best_hs_code[:2] == hs_actual[:2]:\n",
    "            correct_two_digits += 1\n",
    "\n",
    "        # Check 4-digit accuracy\n",
    "        if best_hs_code == hs_actual:\n",
    "            correct_four_digits += 1\n",
    "\n",
    "    accuracy_at_two_digits = correct_two_digits / total if total > 0 else 0\n",
    "    accuracy_at_four_digits = correct_four_digits / total if total > 0 else 0\n",
    "\n",
    "    return accuracy_at_two_digits, accuracy_at_four_digits\n",
    "\n",
    "n_iterations = 2\n",
    "n_predictions = 30\n",
    "accuracy_results_list = []\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Extract the queries from the 'Raw_data_input' column\n",
    "all_queries = df[['Raw_data_input', 'Expected_output','Expected_output_two_digits', 'Expected_output_four_digits']].dropna()  # Ensure no missing values\n",
    "\n",
    "\n",
    "#Montecarlo simulation\n",
    "#for iteration in tqdm(range(n_iterations), desc=\"Running MonteCarlo simulation\", unit=\"iteration\"):\n",
    "#    #print(\"Running iteration #:\" , iteration+1)\n",
    "#    iteration_df = run_predictions(n_predictions)\n",
    "#    # Calculate accuracy\n",
    "#    acc_two_digits, acc_four_digits = calculate_accuracy(iteration_df)\n",
    "#\n",
    "#    # Store the results\n",
    "#    accuracy_results_list.append([iteration + 1, acc_two_digits, acc_four_digits])\n",
    "    #print(\"Accuracy of this iteration: \", acc_two_digits, acc_four_digits)\n",
    "## Convert results to DataFrame for further analysis\n",
    "#accuracy_results_df = pd.DataFrame(accuracy_results_list, columns=['Iteration', 'Accuracy_At_Two_Digits', 'Accuracy_At_Four_Digits'])\n",
    "#accuracy_results_df\n",
    "\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#accuracy_results_df[['Accuracy_At_Two_Digits', 'Accuracy_At_Four_Digits']].boxplot()\n",
    "#plt.title('Monte Carlo Simulation: Accuracy at Two and Four Digits')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "##### Run 100% of dataset once\n",
    "acc_two_digits, acc_four_digits = calculate_accuracy(run_predictions(100))\n",
    "print(f\"Accuracy achieved in 100% of the dataset: {acc_two_digits * 100:.2f}% (2-digit), {acc_four_digits * 100:.2f}% (4-digit)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
