{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\maestria\\Proyecto Integrador\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "#import dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "from rich import print\n",
    "import random, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "\n",
    "# Download necessary NLTK data (run this only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CUDA Available: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CUDA Available: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device Name: NVIDIA GeForce RTX <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3050</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Device Name: NVIDIA GeForce RTX \u001b[1;36m3050\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activando CUDA para acelerar el procesamiento de vectores ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_name = 'TestC'\n",
    "LLM_model = 'llama3.1'\n",
    "temperature_parameter=0.08\n",
    "retriever_matches_k = 3\n",
    "\n",
    "## Save each iteration to file\n",
    "output_file_path = '../Predicciones_ModeloFinal_ValidationDataset.csv'\n",
    "\n",
    "# Load the CSV validation file\n",
    "df = pd.read_csv('../validation_dataset.csv', encoding='utf-8', dtype=str)\n",
    "#df = pd.read_csv('../data validation verified.csv', encoding='utf-8', dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir =  os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir,'..' ,'index', index_test_name))\n",
    "\n",
    "embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Para correr las predicciones en GPU\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea la estructura del RAG\n",
    "Utiliza HuggingFaceInstructEmbeddings para generar embeddings de descripciones de productos.\n",
    "\n",
    "Almacena y gestiona embeddings en una base de datos vectorial Chroma.\n",
    "\n",
    "Recupera documentos más relevantes basados en similitud con un retriever ajustado a los mejores 3 resultados.\n",
    "\n",
    "Utiliza el modelo de lenguaje Llama3 para predecir el código HS más preciso a partir de descripciones.\n",
    "\n",
    "Limita la respuesta a un único código HS, extraído del documento con mayor similitud.\n",
    "\n",
    "Define una plantilla personalizada para guiar las respuestas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "##### Embedding model (sentence-transformer)\n",
    "model_name = embed_model\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "##### LLM model\n",
    "# loading the Llama3 model from local\n",
    "llm_model = OllamaLLM(model=LLM_model,\n",
    "                temperature=temperature_parameter,\n",
    "                num_thread=8,\n",
    "                )\n",
    "# loading the vectorstore\n",
    "vectorstore = Chroma(persist_directory=persistent_dir, embedding_function=hf_embed_model)\n",
    "# casting  the vectorstore as the retriever, taking the best 3 similarities\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":retriever_matches_k})\n",
    "\n",
    "# formating the docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#template = \"\"\"You must respond only with the hs_code from the metadata of the source_documents retrieved by the retriever.\n",
    "#with the highest similarity score, avoid responding with any kind of text \n",
    "#even if you can be sure if the HS code is correct like \n",
    "#\"I'm unable to provide a response...\" or \"I cannot provide a response\".\n",
    "\n",
    "\n",
    "template = \"\"\"You must respond only with the best HS code which is contained as 'hs_code' on the metadata of the source_documents you're receiving.\n",
    "Avoid responding with any other text. Respond only with 1 HS code, the one in better score\n",
    "\n",
    "context:\n",
    "{summaries}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Define the LLM chain (using the Llama3.1 model)\n",
    "llm_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm_model,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # To get both the answer and source docs\n",
    "    chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                #For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "                input_variables=[\"question\", \"summaries\"],\n",
    "            ),\n",
    "        },\n",
    ")\n",
    "\n",
    "context = \"\"\"As a logistics shipping arrival inspector, your primary responsibility is to inspect incoming shipments and accurately classify goods \n",
    "using the Harmonized System (HS) code based on the descriptions provided in the shipping manifests. You will thoroughly review the manifest details, \n",
    "including product type, material composition, function, and intended use, to determine the correct HS code. \n",
    "\n",
    "Your task is to:\n",
    "Carefully read and analyze the product descriptions from the manifest.\n",
    "Identify key characteristics of the goods, such as \n",
    "type (e.g., electronics, textiles, machinery), \n",
    "material (e.g., plastic, metal, organic), \n",
    "and usage (e.g., household, industrial, medical).\n",
    "Use your knowledge of the HS code classification system to assign the most appropriate HS code for each product based on its description.\n",
    "Ensure compliance with international trade regulations by selecting precise codes to avoid delays or penalties.\n",
    "Remember to be thorough and accurate in your classification, as this impacts customs processing, tariffs, and legal requirements.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de descripciones 'Hardcoded' para verificar viabilidad de corrida Montecarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#######  Option 2 - Stopwords technique. Simpler but better results\n",
    "# Clean text function to avoid garbage\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert text to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove non-alphanumeric characters (keep spaces)\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [word for word in words if word not in combined_stopwords]     # Remove stopwords\n",
    "    # Lemmatization to convert the received sentence to a meaningful sentence\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join words back into a sentence\n",
    "    meaningful_text = ' '.join(words)\n",
    "    return meaningful_text \n",
    "\n",
    "# Definir stopwords en español y las stopwords adicionales específicas del contexto\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "additional_stopwords = {\n",
    "    'hs', 'code', 'hscode', 'hs-code', 'hs  code', 'pallets', 'plts', 'shipper', 'declares', 'hs code',\n",
    "    'containing', 'contains', 'meter', 'cubic', 'packages', 'load', 'loaded', 'weight', \n",
    "    'netweight', 'kg', 'kgs', 'cb', 'cbm', 'goods', 'parts', 'pieces', 'accessories', 'packing', \n",
    "    'declared', 'dangerous', 'impression', 'items', 'sheets', 'codes', \n",
    "    'sin', 'impresion', 'containers', 'pc', 'abv', 'net', 'gross', 'cif', 'aduana', 'customs', \n",
    "    'value', 'tax', 'duty', 'freight', 'port', 'terminal', 'consignee', 'consignor', 'invoice', \n",
    "    'manifest', 'quantity', 'description', 'volume', 'packaging', 'shipment', 'delivery', 'origin', \n",
    "    'destination', 'transport', 'carrier', 'export', 'import', 'tariff', 'item', 'declaration', \n",
    "    'clearance', 'documentation', 'commercial', 'charge', 'fees', 'logistics', 'shipping', \n",
    "    'container', 'unit', 'measurement', 'certification', 'palletized', 'metric', 'commodity', \n",
    "    'classification', 'entry', 'exportation', 'importation', 'bonded', 'zone', 'trade', 'license', 'bottle', 'bottles', 'cl',\n",
    "    'ancho', 'largo', 'mm', 'pcs', 'xhc', 'stc', 'uks','x','k', 'pty', 'id', 'cp', 'ncm', 'ne', 'itpa', 'zz', 'xg', 'topmag',\n",
    "    'rtmx', 'fcl', 'cf','f', 'xdc', 'pkgs'\n",
    "}\n",
    "\n",
    "# Combinar stopwords estándar con stopwords del contexto específico\n",
    "combined_stopwords = stop_words_es.union(stop_words_en).union(additional_stopwords)\n",
    "\n",
    "#query = \"sodium lignosulphonate\"\n",
    "#query = \"kilogram organic coffee bean certified fairtrade utz roasted usa global distribution\"\n",
    "query = \"prepaid cartons wine\"\n",
    "#query = \"wine\"\n",
    "#query = \"xhr said contain package sudu reef shipper seal kn package total versatis mg parches lot r temperature must main taine\"\n",
    "\n",
    "\n",
    "# Execute without cleaning\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "#result = llm_chain({\"question\": query, \"summaries\": context})\n",
    "#print(f\"[bold yellow]The requested item's description RAW to search HTS code is:[/bold yellow]\\n{query}\")\n",
    "#print(f\"[bold green]The response of the LLM is:[/bold green]\\n{result['answer']}\")\n",
    "\n",
    "# Execute cleaning\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "result = llm_chain({\"question\": clean_text(query), \"summaries\": context})\n",
    "print(f\"[bold yellow]The requested item's description CLEANED to search HTS code is:[/bold yellow]\\n{clean_text(query)}\")\n",
    "print(f\"[bold green]The response of the LLM is:[/bold green]\\n{result['answer']}\")\n",
    "\n",
    "#print(result.keys())\n",
    "#print(result[\"summaries\"])\n",
    "print(result[\"sources\"])\n",
    "\n",
    "print(\"The documents sent to the LLM are:\")\n",
    "for i in range(len(result[\"source_documents\"])):\n",
    "    print(result[\"source_documents\"][i].page_content)\n",
    "    print(result[\"source_documents\"][i].metadata)\n",
    "    \n",
    "results = vectorstore.similarity_search_with_score(query=query, k=retriever_matches_k)\n",
    "#print(\"Top n coincidences from the index are:\")\n",
    "#for doc, score in results:\n",
    "#    print(f\"Document content: {doc.page_content}, Code: {doc.metadata},Similarity Score: {score}\")\n",
    "\n",
    "\n",
    "# Find the document with the highest score\n",
    "max_doc, max_score = max(results, key=lambda x: x[1])\n",
    "print(\"Best match from the index is:\")\n",
    "print(f\"Document content: {max_doc.page_content}, Code: {max_doc.metadata}, Similarity Score: {max_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrida montecarlo, 50 iteraciones de muestreos de 30 despcripciones cada una. \n",
    "\n",
    "El resultado se vacia en un csv para verificacion detallada de cada codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(descriptions_to_predict):\n",
    "    # Select 30 random queries\n",
    "    # Shuffle the original DataFrame\n",
    "    df_shuffled = all_queries.sample(frac=1).reset_index(drop=True)\n",
    "    selected_queries = df_shuffled.sample(descriptions_to_predict)\n",
    "\n",
    "    # Now, process each query with llm_chain\n",
    "    #print(\"Predicting HS codes of random Validation Data rows...\")\n",
    "    for index, row in selected_queries.iterrows():\n",
    "        query = clean_text(row['Raw_data_input'])\n",
    "        expected_output = row['Expected_output']\n",
    "\n",
    "        # Execute the LLM chain (using a mock prediction for illustration here)\n",
    "        result = llm_chain({\"question\": query, \"summaries\": context})  # This is the actual call\n",
    "        llm_prediction = result['answer']  # Assuming LLM returns a numeric 6-digit value as the prediction\n",
    "\n",
    "        # Append the result in the required structure\n",
    "        results.append({\n",
    "            \"Raw_data_input\": row['Raw_data_input'],\n",
    "            \"Raw_data_input_processed\": query,\n",
    "            \"LLM_prediction\": llm_prediction,  # Here, the prediction from LLM\n",
    "            \"Expected_output\": expected_output,\n",
    "            \"Expected_output_two_digits\": row['Expected_output_two_digits'],\n",
    "            \"Predicted_output_two_digits\": llm_prediction[:2],\n",
    "            \"CorrectMatch_two_digits\": 1 if llm_prediction[:2] == row['Expected_output_two_digits'] else 0 ,\n",
    "            \"Expected_output_four_digits\": row['Expected_output_four_digits'],\n",
    "            \"Predicted_output_four_digits\": llm_prediction[:4],\n",
    "            \"CorrectMatch_four_digits\": 1 if llm_prediction[:4] == row['Expected_output_four_digits'] else 0 \n",
    "        })\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    predictions_df = pd.DataFrame(results)\n",
    "\n",
    "    #Append iteration results to file\n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(output_file_path)\n",
    "\n",
    "    # Append to the file, write header only if file doesn't already exist\n",
    "    predictions_df.to_csv(output_file_path, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "# Function to calculate accuracy (both for 2 and 4 digits)\n",
    "def calculate_accuracy(iteration_df):\n",
    "    correct_two_digits = 0\n",
    "    correct_four_digits = 0\n",
    "    total = len(iteration_df)\n",
    "\n",
    "    for index, row in iteration_df.iterrows():\n",
    "        best_hs_code = str(row['LLM_prediction'])[:4]\n",
    "        hs_actual = str(row['Expected_output'])[:4]\n",
    "\n",
    "        # Check 2-digit accuracy\n",
    "        if best_hs_code[:2] == hs_actual[:2]:\n",
    "            correct_two_digits += 1\n",
    "\n",
    "        # Check 4-digit accuracy\n",
    "        if best_hs_code == hs_actual:\n",
    "            correct_four_digits += 1\n",
    "\n",
    "    accuracy_at_two_digits = correct_two_digits / total if total > 0 else 0\n",
    "    accuracy_at_four_digits = correct_four_digits / total if total > 0 else 0\n",
    "\n",
    "    return accuracy_at_two_digits, accuracy_at_four_digits\n",
    "\n",
    "n_iterations = 50\n",
    "n_predictions = 30\n",
    "accuracy_results_list = []\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Extract the queries from the 'Raw_data_input' column\n",
    "all_queries = df[['Raw_data_input', 'Expected_output','Expected_output_two_digits', 'Expected_output_four_digits']].dropna()  # Ensure no missing values\n",
    "\n",
    "for iteration in tqdm(range(n_iterations), desc=\"Running MonteCarlo simulation\"):\n",
    "    #print(\"Running iteration #:\" , iteration+1)\n",
    "    iteration_df = run_predictions(n_predictions)\n",
    "    # Calculate accuracy\n",
    "    acc_two_digits, acc_four_digits = calculate_accuracy(iteration_df)\n",
    "\n",
    "    # Store the results\n",
    "    accuracy_results_list.append([iteration + 1, acc_two_digits, acc_four_digits])\n",
    "    #print(\"Accuracy of this iteration: \", acc_two_digits, acc_four_digits)\n",
    "\n",
    "# Convert results to DataFrame for further analysis\n",
    "accuracy_results_df = pd.DataFrame(accuracy_results_list, columns=['Iteration', 'Accuracy_At_Two_Digits', 'Accuracy_At_Four_Digits'])\n",
    "accuracy_results_df\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_results_df[['Accuracy_At_Two_Digits', 'Accuracy_At_Four_Digits']].boxplot()\n",
    "plt.title('Monte Carlo Simulation: Accuracy at Two and Four Digits')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
