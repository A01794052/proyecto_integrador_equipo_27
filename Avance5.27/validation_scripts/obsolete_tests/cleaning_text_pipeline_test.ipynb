{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "import warnings\n",
    "from rich import print\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import yake\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words, stopwords\n",
    "import spacy\n",
    "\n",
    "# Download necessary NLTK data (run this only once)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">medical face maskn ahs sme t ext jonathan liedot bollore brokere\n",
       "</pre>\n"
      ],
      "text/plain": [
       "medical face maskn ahs sme t ext jonathan liedot bollore brokere\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up YAKE keyword extractor\n",
    "yake_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.1\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "\n",
    "# Load the translation model (use Spanish-to-English model)\n",
    "model_name = 'Helsinki-NLP/opus-mt-es-en'  # Spanish to English model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Get the English words corpus and English stopwords\n",
    "english_words = set(words.words())\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "# Add your custom stopwords\n",
    "additional_stopwords = {\n",
    "    # Add your custom stopwords here (same as your provided list)\n",
    "    'hs', 'code', 'hscode', 'hs-code', 'hs  code', 'pallets', 'plts', 'shipper', 'declares', 'hs code',\n",
    "    'containing', 'contains', 'meter', 'cubic', 'packages', 'load', 'loaded', 'weight', \n",
    "    'netweight', 'kg', 'kgs', 'cb', 'cbm', 'goods', 'parts', 'pieces', 'accessories', 'packing', \n",
    "    'declared', 'dangerous', 'impression', 'items', 'sheets', 'codes', \n",
    "    'sin', 'impresion', 'containers', 'pc', 'abv', 'net', 'gross', 'cif', 'aduana', 'customs', \n",
    "    'value', 'tax', 'duty', 'freight', 'port', 'terminal', 'consignee', 'consignor', 'invoice', \n",
    "    'manifest', 'quantity', 'description', 'volume', 'packaging', 'shipment', 'delivery', 'origin', \n",
    "    'destination', 'transport', 'carrier', 'export', 'import', 'tariff', 'item', 'declaration', \n",
    "    'clearance', 'documentation', 'commercial', 'charge', 'fees', 'logistics', 'shipping', \n",
    "    'container', 'unit', 'measurement', 'certification', 'palletized', 'metric', 'commodity', \n",
    "    'classification', 'entry', 'exportation', 'importation', 'bonded', 'zone', 'trade', 'license', 'bottle', 'bottles', 'cl',\n",
    "    'ancho', 'largo', 'mm', 'pcs', 'xhc', 'stc', 'uks','x','k', 'pty', 'id', 'cp', 'ncm', 'ne', 'itpa', 'zz', 'xg', 'topmag',\n",
    "    'rtmx', 'fcl', 'cf','f', 'xdc', 'pkgs', 'voice', 'n', 'per', 'email', 'phone', 'fax', 'tax', 'id', 'sms', \n",
    "    'tel', 'mobile', 'cell', 'cellular', 'mobile', 'mobiles', 'telephone', 'fax', 'email', 'mail', 'mails', 'email', \n",
    "    'emails', 'faxes', 'emails', 'fax', 'faxes', 'phones', 'phone', 'tel', 'tels', 'telephone', 'telephones', 'cell', \n",
    "    'cellular', 'cellulars', 'mobile', 'mobiles', 'prepaid'\n",
    "}\n",
    "\n",
    "stop_words = english_stopwords.union(additional_stopwords)\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_translate_clean(text):\n",
    "    keywords = text.split()\n",
    "    valid_keywords = [keyword for keyword in keywords if keyword not in additional_stopwords]\n",
    "    lematized_keywords = [lemmatizer.lemmatize(keyword) for keyword in valid_keywords]\n",
    "\n",
    "    ### Lemmatize the extracted keywords\n",
    "    ##lemmatized_keywords = [lemmatizer.lemmatize(keyword[0]) for keyword in keywords]\n",
    "    lemmatized_sentence = \" \".join(lematized_keywords)\n",
    "\n",
    "    ### Correct typos in the lemmatized keywords\n",
    "    ##corrected_keywords = [str(TextBlob(keyword).correct()) for keyword in lemmatized_keywords]\n",
    "    ##lematized_and_corrected_typos_sentence = \" \".join(corrected_keywords)\n",
    "\n",
    "    ##lemmatize_translate_clean_text = []\n",
    "    ##flattened_items = [word for item in lemmatized_keywords for word in item.split()]\n",
    "    ###print(flattened_items)\n",
    "    ##for item in flattened_items:\n",
    "    ##    lemma = lemmatizer.lemmatize(item)\n",
    "    ##    #Removing condition to check if the word exists on english dictionary, cartulin was not there for example\n",
    "    ##    #if lemma in english_words and lemma not in english_stopwords:\n",
    "    ##    if lemma in lemma not in english_stopwords:            \n",
    "    ##        lemmatize_translate_clean_text.append(lemma)\n",
    "    #print(english_existing_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[\\d]+|[^\\w\\s]', '', text)  # Remove numbers and punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text, tokenizer, model):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Generate translation\n",
    "    translated = model.generate(**inputs)\n",
    "    # Decode and return the translated text\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "def get_synonyms(term):\n",
    "    synonyms = set()\n",
    "    \n",
    "    # Get synsets for the word\n",
    "    for synset in wn.synsets(term):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores with spaces for readability\n",
    "    return synonyms\n",
    "\n",
    "def extract_nouns(phrase):\n",
    "    doc = nlp(phrase)  # Process the input phrase\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]  # Extract nouns\n",
    "    print(\"Nouns are:\", nouns)\n",
    "    return nouns\n",
    "\n",
    "def extract_adjectives(phrase):\n",
    "    # Process the phrase using spaCy\n",
    "    doc = nlp(phrase)\n",
    "    # Extract words that are adjectives (POS tag 'ADJ')\n",
    "    adjectives = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "    print(\"Adjectives are:\", adjectives)\n",
    "    return adjectives\n",
    "\n",
    "# Full pipeline function\n",
    "def process_text(text):\n",
    "    try:\n",
    "        # Step 1: Preprocess\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        \n",
    "        # Step 2: Translate\n",
    "        translated_text = translate_to_english(preprocessed_text, tokenizer, model)\n",
    "        \n",
    "        # Step 3: Lemmatize, remove nonsense words, and clean\n",
    "        #final_cleaned_text = lemmatize_and_clean(translated_text)\n",
    "        final_cleaned_text = lemmatize_translate_clean(translated_text)\n",
    "\n",
    "        #extract_nouns(\" \".join(final_cleaned_text))\n",
    "        #extract_adjectives(\" \".join(final_cleaned_text))\n",
    "        return final_cleaned_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "# Example usage\n",
    "#input_text = \"Este es un texto en espa√±ol que queremos traducir y limpiar.\"\n",
    "#input_text = \"said contain plastic drum h msku dry shipper seal pw plastic drum h stowed methyl butyric acid corrosive liqu\"\n",
    "#input_text = \"TUBOS 7306.3011 TUBOS Y ACCESORIOS .1990-7307.9980 RAMPAS .9000 TAPON  TUERCA DE LATON .3300 ACCESORIOS DE TUBERIA DE ALUMINIO .0000\"\n",
    "#input_text = \"xhr said contain package sudu reef shipper seal kn package total versatis mg parches lot r temperature must main taine\"\n",
    "#input_text = \"containerstotal boxesporcelain tile glazed size exp dt bill dt wt kgsfreight collectthis master bill\"\n",
    "#input_text = \"CARTULINA NEGRA BOB 170 GRS HS CODE 480258 CARTULINA BLANCA BOB 210 GRS HS CODE 480258\"\n",
    "#input_text = \"GYM EQUIPMENT AS PER COMMERCIAL INVOICE N VXUSD210182HS CODE  950691FREIGHT PREPAIDGYM EQUIPMENT AS PER COMMERCIAL INVOICE N VXUSD210182HS CODE  95069\"\n",
    "#input_text = \"HERRAJES PARA MUEBLES HS CODE 83024200 EMAIL G.GIACOMO RAGO-GROUP.COM\"\n",
    "input_text = \"DISPOSABLE MEDICAL FACE MASKN AHS CODE  630790 FAX EMAIL TAX ID 9144010178376732X5  TAX ID  SME980702916 T   52  55 4334 7000EXT 7043PHONE  FAX EMAIL  JONATHAN LIEDOT BOLLORE COMFREIGHT PREPAID\"\n",
    "cleaned_output = process_text(input_text)\n",
    "print(cleaned_output)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract nouns and search synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "\n",
    "# Download necessary NLTK data (only needed once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to fetch synonyms from WordNet\n",
    "def get_synonyms(term):\n",
    "    synonyms = set()\n",
    "    \n",
    "    # Get synsets for the word\n",
    "    for synset in wn.synsets(term):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores with spaces for readability\n",
    "    return synonyms\n",
    "\n",
    "def extract_nouns(phrase):\n",
    "    doc = nlp(phrase)  # Process the input phrase\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]  # Extract nouns\n",
    "    return nouns\n",
    "\n",
    "input_text = \"CARTULINA NEGRA BOB 170 GRS HS CODE 480258 CARTULINA BLANCA BOB 210 GRS HS CODE 480258\"\n",
    "cleaned_output = process_text(input_text)\n",
    "nouns = extract_nouns(cleaned_output)\n",
    "\n",
    "print(f\"Nouns extracted from '{cleaned_output}': {nouns}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
