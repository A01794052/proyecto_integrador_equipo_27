{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\maestria\\Proyecto Integrador\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "#import dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "from rich import print\n",
    "import random, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "\n",
    "# Download necessary NLTK data (run this only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CUDA Available: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CUDA Available: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Device Name: NVIDIA GeForce RTX <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3050</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Device Name: NVIDIA GeForce RTX \u001b[1;36m3050\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for this test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_name = 'TestF'\n",
    "LLM_model = 'llama3.1'\n",
    "temperature_parameter=0.08\n",
    "retriever_matches_k = 3\n",
    "\n",
    "## Save each iteration to file\n",
    "output_file_path = '../Prediction_vs_Expected_TestF.csv'\n",
    "\n",
    "# Load the CSV validation file\n",
    "df = pd.read_csv('../validation_dataset.csv', encoding='utf-8', dtype=str)\n",
    "#df = pd.read_csv('../data validation verified.csv', encoding='utf-8', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir =  os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir,'..' ,'index', index_test_name))\n",
    "\n",
    "# SE REQUIERE UTILIZAR EL MISMO MODELO DE SENTENCE-TRANSFORMER CON EL QUE SE CREO EL INDEX, sin ello se enfrentaran:\n",
    "#Espacios Vectoriales Diferentes:\n",
    "#\n",
    "#Cada modelo de sentence-transformer genera embeddings en su propio espacio vectorial. Estos espacios son definidos por los pesos y arquitecturas específicas de cada modelo.\n",
    "#Los embeddings generados por el modelo A no son directamente comparables con los del modelo B porque están en espacios diferentes y no alineados.\n",
    "#Incompatibilidad de Embeddings:\n",
    "#\n",
    "#Las representaciones vectoriales (embeddings) de las mismas frases pueden ser muy distintas entre modelos. Por ejemplo, una oración podría tener un vector [0.1, 0.2, 0.3] en el modelo A y [0.5, -0.1, 0.2] en el modelo B.\n",
    "#Esto significa que medidas de similitud como la similitud coseno no serán significativas, ya que los vectores no están en el mismo espacio.\n",
    "#Resultados Incorrectos o Sin Sentido:\n",
    "#\n",
    "#Al realizar consultas, las comparaciones entre embeddings del índice (modelo A) y los embeddings de la consulta (modelo B) producirán resultados erróneos.\n",
    "#Podrías obtener altas similitudes entre oraciones no relacionadas o bajas similitudes entre oraciones muy similares.\n",
    "#Pérdida de Precisión y Rendimiento:\n",
    "#\n",
    "#El rendimiento del sistema de recuperación de información se degradará significativamente.\n",
    "#Los usuarios recibirán resultados irrelevantes, lo que afecta la usabilidad y confiabilidad del sistema.\n",
    "#Consistencia en Procesamiento de Lenguaje Natural:\n",
    "#\n",
    "#Los modelos pueden tener diferentes enfoques para manejar ciertos aspectos del lenguaje, como negaciones, sarcasmo o lenguaje coloquial.\n",
    "#Esto añade otra capa de inconsistencia entre los embeddings generados por diferentes modelos.\n",
    "\n",
    "#embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Para correr las predicciones en GPU\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a Q&A LLM Chain to provide context and system prompts on every query\n",
    "This will run peristantly, consulting already stored vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "##### Embedding model (sentence-transformer)\n",
    "model_name = embed_model\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "##### LLM model\n",
    "# loading the Llama3 model from local\n",
    "llm_model = OllamaLLM(model=LLM_model,\n",
    "                temperature=temperature_parameter,\n",
    "                num_thread=8,\n",
    "                )\n",
    "# loading the vectorstore\n",
    "vectorstore = Chroma(persist_directory=persistent_dir, embedding_function=hf_embed_model)\n",
    "# casting  the vectorstore as the retriever, taking the best 3 similarities\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":retriever_matches_k})\n",
    "\n",
    "# formating the docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "template = \"\"\"You must respond only with the best HS code which is contained as 'hs_code' on the metadata of the source_documents you're receiving.\n",
    "Avoid responding with any other text. Respond only with 1 HS code, the one in better score\n",
    "\n",
    "\n",
    "context:\n",
    "{summaries}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Define the LLM chain (using the Llama3.1 model)\n",
    "llm_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm_model,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # To get both the answer and source docs\n",
    "    chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                #For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "                input_variables=[\"question\", \"summaries\"],\n",
    "            ),\n",
    "        },\n",
    ")\n",
    "\n",
    "context = \"\"\"As a logistics shipping arrival inspector, your primary responsibility is to inspect incoming shipments and accurately classify goods \n",
    "using the Harmonized System (HS) code based on the descriptions provided in the shipping manifests. You will thoroughly review the manifest details, \n",
    "including product type, material composition, function, and intended use, to determine the correct HS code. \n",
    "\n",
    "Your task is to:\n",
    "Carefully read and analyze the product descriptions from the manifest.\n",
    "Identify key characteristics of the goods, such as \n",
    "type (e.g., electronics, textiles, machinery), \n",
    "material (e.g., plastic, metal, organic), \n",
    "and usage (e.g., household, industrial, medical).\n",
    "Use your knowledge of the HS code classification system to assign the most appropriate HS code for each product based on its description.\n",
    "Ensure compliance with international trade regulations by selecting precise codes to avoid delays or penalties.\n",
    "Remember to be thorough and accurate in your classification, as this impacts customs processing, tariffs, and legal requirements.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running isolated test to check example with and without lemmatization/spelling check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#######  Option 2 - Stopwords technique. Simpler but better results\n",
    "# Clean text function to avoid garbage\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert text to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove non-alphanumeric characters (keep spaces)\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    #words = [word for word in words if word not in combined_stopwords]     # Remove stopwords\n",
    "    # Lemmatization to convert the received sentence to a meaningful sentence\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join words back into a sentence\n",
    "    meaningful_text = ' '.join(words)\n",
    "\n",
    "    ## Extraer solo palabras que existen en ingles\n",
    "    return meaningful_text \n",
    "\n",
    "# Definir stopwords en español y las stopwords adicionales específicas del contexto\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "additional_stopwords = {\n",
    "    'hs', 'code', 'hscode', 'hs-code', 'hs  code', 'pallets', 'plts', 'shipper', 'declares', 'hs code',\n",
    "    'containing', 'contains', 'meter', 'cubic', 'packages', 'load', 'loaded', 'weight', \n",
    "    'netweight', 'kg', 'kgs', 'cb', 'cbm', 'goods', 'parts', 'pieces', 'accessories', 'packing', \n",
    "    'declared', 'dangerous', 'impression', 'items', 'sheets', 'codes', \n",
    "    'sin', 'impresion', 'containers', 'pc', 'abv', 'net', 'gross', 'cif', 'aduana', 'customs', \n",
    "    'value', 'tax', 'duty', 'freight', 'port', 'terminal', 'consignee', 'consignor', 'invoice', \n",
    "    'manifest', 'quantity', 'description', 'volume', 'packaging', 'shipment', 'delivery', 'origin', \n",
    "    'destination', 'transport', 'carrier', 'export', 'import', 'tariff', 'item', 'declaration', \n",
    "    'clearance', 'documentation', 'commercial', 'charge', 'fees', 'logistics', 'shipping', \n",
    "    'container', 'unit', 'measurement', 'certification', 'palletized', 'metric', 'commodity', \n",
    "    'classification', 'entry', 'exportation', 'importation', 'bonded', 'zone', 'trade', 'license', 'bottle', 'bottles', 'cl',\n",
    "    'ancho', 'largo', 'mm', 'pcs', 'xhc', 'stc', 'uks','x','k', 'pty', 'id', 'cp', 'ncm', 'ne', 'itpa', 'zz', 'xg', 'topmag',\n",
    "    'rtmx', 'fcl', 'cf','f', 'xdc', 'pkgs'\n",
    "}\n",
    "\n",
    "# Combinar stopwords estándar con stopwords del contexto específico\n",
    "combined_stopwords = stop_words_es.union(stop_words_en).union(additional_stopwords)\n",
    "\n",
    "\n",
    "#query = \"sodium lignosulphonate\"\n",
    "#query = \"kilogram organic coffee bean certified fairtrade utz roasted usa global distribution\"\n",
    "#query = \"prepaid cartons wine\"\n",
    "#query = \"wine\"\n",
    "#query = \"aluminium strip\"\n",
    "#query = \"a live dog\"\n",
    "#query = \"xhr said contain package sudu reef shipper seal kn package total versatis mg parches lot r temperature must main taine\"\n",
    "query = \"volvo related busparts prepad fax vat vim n admcd\"\n",
    "\n",
    "# Execute without cleaning\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "result = llm_chain({\"question\": query, \"summaries\": context})\n",
    "print(f\"[bold yellow]The requested item's description RAW to search HTS code is:[/bold yellow]\\n{query}\")\n",
    "print(f\"[bold green]The response of the LLM is:[/bold green]\\n{result['answer']}\")\n",
    "\n",
    "# Execute cleaning\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "result = llm_chain({\"question\": clean_text(query), \"summaries\": context})\n",
    "print(f\"[bold yellow]The requested item's description CLEANED to search HTS code is:[/bold yellow]\\n{clean_text(query)}\")\n",
    "print(f\"[bold green]The response of the LLM is:[/bold green]\\n{result['answer']}\")\n",
    "\n",
    "print(result.keys())\n",
    "#print(result[\"summaries\"])\n",
    "print(result[\"sources\"])\n",
    "\n",
    "print(\"The documents sent to the LLM are:\")\n",
    "for i in range(len(result[\"source_documents\"])):\n",
    "    print(result[\"source_documents\"][i].page_content)\n",
    "    print(result[\"source_documents\"][i].metadata)\n",
    "    \n",
    "#results = vectorstore.similarity_search_with_score(query=query, k=retriever_matches_k)\n",
    "#print(\"Top n coincidences from the index are:\")\n",
    "#for doc, score in results:\n",
    "#    print(f\"Document content: {doc.page_content}, Code: {doc.metadata},Similarity Score: {score}\")\n",
    "\n",
    "\n",
    "# Find the document with the highest score\n",
    "#max_doc, max_score = max(results, key=lambda x: x[1])\n",
    "#print(\"Best match from the index is:\")\n",
    "#print(f\"Document content: {max_doc.page_content}, Code: {max_doc.metadata}, Similarity Score: {max_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 50 iterations of 30 random samples each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(descriptions_to_predict):\n",
    "    # Select 30 random queries\n",
    "    # Shuffle the original DataFrame\n",
    "    df_shuffled = all_queries.sample(frac=1).reset_index(drop=True)\n",
    "    selected_queries = df_shuffled.sample(descriptions_to_predict)\n",
    "\n",
    "    # Now, process each query with llm_chain\n",
    "    #print(\"Predicting HS codes of random Validation Data rows...\")\n",
    "    for index, row in selected_queries.iterrows():\n",
    "        query = clean_text(row['Raw_data_input'])\n",
    "        expected_output = row['Expected_output']\n",
    "\n",
    "        # Execute the LLM chain (using a mock prediction for illustration here)\n",
    "        result = llm_chain({\"question\": query, \"summaries\": context})  # This is the actual call\n",
    "        llm_prediction = result['answer']  # Assuming LLM returns a numeric 6-digit value as the prediction\n",
    "\n",
    "        # Append the result in the required structure\n",
    "        results.append({\n",
    "            \"Raw_data_input\": row['Raw_data_input'],\n",
    "            \"Raw_data_input_processed\": query,\n",
    "            \"LLM_prediction\": llm_prediction,  # Here, the prediction from LLM\n",
    "            \"Expected_output\": expected_output,\n",
    "            \"Expected_output_two_digits\": row['Expected_output_two_digits'],\n",
    "            \"Predicted_output_two_digits\": llm_prediction[:2],\n",
    "            \"CorrectMatch_two_digits\": 1 if llm_prediction[:2] == row['Expected_output_two_digits'] else 0 ,\n",
    "            \"Expected_output_four_digits\": row['Expected_output_four_digits'],\n",
    "            \"Predicted_output_four_digits\": llm_prediction[:4],\n",
    "            \"CorrectMatch_four_digits\": 1 if llm_prediction[:4] == row['Expected_output_four_digits'] else 0 \n",
    "        })\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    predictions_df = pd.DataFrame(results)\n",
    "\n",
    "    #Append iteration results to file\n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(output_file_path)\n",
    "\n",
    "    # Append to the file, write header only if file doesn't already exist\n",
    "    predictions_df.to_csv(output_file_path, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "# Function to calculate accuracy (both for 2 and 4 digits)\n",
    "def calculate_accuracy(iteration_df):\n",
    "    correct_two_digits = 0\n",
    "    correct_four_digits = 0\n",
    "    total = len(iteration_df)\n",
    "\n",
    "    for index, row in iteration_df.iterrows():\n",
    "        best_hs_code = str(row['LLM_prediction'])[:4]\n",
    "        hs_actual = str(row['Expected_output'])[:4]\n",
    "\n",
    "        # Check 2-digit accuracy\n",
    "        if best_hs_code[:2] == hs_actual[:2]:\n",
    "            correct_two_digits += 1\n",
    "\n",
    "        # Check 4-digit accuracy\n",
    "        if best_hs_code == hs_actual:\n",
    "            correct_four_digits += 1\n",
    "\n",
    "    accuracy_at_two_digits = correct_two_digits / total if total > 0 else 0\n",
    "    accuracy_at_four_digits = correct_four_digits / total if total > 0 else 0\n",
    "\n",
    "    return accuracy_at_two_digits, accuracy_at_four_digits\n",
    "\n",
    "n_iterations = 50\n",
    "n_predictions = 30\n",
    "accuracy_results_list = []\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Extract the queries from the 'Raw_data_input' column\n",
    "all_queries = df[['Raw_data_input', 'Expected_output','Expected_output_two_digits', 'Expected_output_four_digits']].dropna()  # Ensure no missing values\n",
    "\n",
    "for iteration in tqdm(range(n_iterations), desc=\"Running MonteCarlo simulation\"):\n",
    "    #print(\"Running iteration #:\" , iteration+1)\n",
    "    iteration_df = run_predictions(n_predictions)\n",
    "    # Calculate accuracy\n",
    "    acc_two_digits, acc_four_digits = calculate_accuracy(iteration_df)\n",
    "\n",
    "    # Store the results\n",
    "    accuracy_results_list.append([iteration + 1, acc_two_digits, acc_four_digits])\n",
    "    #print(\"Accuracy of this iteration: \", acc_two_digits, acc_four_digits)\n",
    "\n",
    "# Convert results to DataFrame for further analysis\n",
    "accuracy_results_df = pd.DataFrame(accuracy_results_list, columns=['Iteration', 'Accuracy_At_Two_Digits', 'Accuracy_At_Four_Digits'])\n",
    "accuracy_results_df\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_results_df[['Accuracy_At_Two_Digits', 'Accuracy_At_Four_Digits']].boxplot()\n",
    "plt.title('Monte Carlo Simulation: Accuracy at Two and Four Digits')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
