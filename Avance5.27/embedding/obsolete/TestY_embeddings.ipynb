{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_openai  import OpenAIEmbeddings\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import warnings, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up embedding model to use with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model definition\n",
    "A technique for representing text data as numerical vectors, which can be input into machine learning models. The embedding model is responsible for converting text into these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Downloading shards:  50%|█████     | 1/2 [25:23<25:23, 1523.97s/it]Error while downloading from https://cdn-lfs.hf.co/repos/ea/00/ea00943d992c7851ad9f4f4bd094a0397fb5087e0f7cba4ef003018963ea07e3/5734c77cf2ff6482713d474ee9c8791cd712b59dd36b05d096af5a4cb41f3f02?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00002.safetensors%3B+filename%3D%22model-00002-of-00002.safetensors%22%3B&Expires=1730184903&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDE4NDkwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lYS8wMC9lYTAwOTQzZDk5MmM3ODUxYWQ5ZjRmNGJkMDk0YTAzOTdmYjUwODdlMGY3Y2JhNGVmMDAzMDE4OTYzZWEwN2UzLzU3MzRjNzdjZjJmZjY0ODI3MTNkNDc0ZWU5Yzg3OTFjZDcxMmI1OWRkMzZiMDVkMDk2YWY1YTRjYjQxZjNmMDI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kghSTzMWWDIdmreX1sN2ZDMOwLzxFD35nxxl73mIGUV2DsF57Gg6jPOYUEyK3AhoN8L3e95wC57hiSnkUjC92U-jUqw9PQZXsIP9S7nksbKMpdlTCihwN4PDN5qgwBigEGCTFeXRkseN0z6ChsfoypdBFMx-4-PUAWkGooulpMkLqE-3qzsB37YJjYVZ2rkJjVFZTRNYuFdbu7oGmFFhQsKBuFRk7bA0Kg66EeGYvbQY0YvuEt8lxMBEZQvV4gDbpAMszNuforLWvcvRLf5dbehyB3Na1ScpCi2iM8AjZYmsn22vTudXFKMX7N9ZMtweO0UypoIIdc8AnDADrBDVwg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Downloading shards: 100%|██████████| 2/2 [37:38<00:00, 1129.43s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", load_in_4bit=True)\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"intfloat/e5-mistral-7b-instruct\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "#\n",
    "#model_embeddings = SentenceTransformerEmbeddings(model_name=\"mistralai/Mistral-7B-Instruct-v0.1\", model_kwargs={\"trust_remote_code\":True}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading embedding model: stat: path should be string, bytes, os.PathLike or integer, not MistralForCausalLM\n"
     ]
    }
   ],
   "source": [
    "# WARNING! :Only runs with this version\n",
    "###### !pip install sentence-transformers==2.2.2  ######\n",
    "#Define the sentence-transformer model:\n",
    "\n",
    "#For English\n",
    "#sentence-transformers/LaBSE\n",
    "#embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#embed_model = \"intfloat/e5-mistral-7b-instruct\"\n",
    "#embed_model = \"Jaume/gemma-2b-embeddings\"\n",
    "\n",
    "\n",
    "#For Spanish \n",
    "#projecte-aina/aguila-7b\n",
    "#embed_model = \"hiiamsid/sentence_similarity_spanish_es\"\n",
    "\n",
    "#Other sentence-transformer settings\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "\n",
    "# Initialize the embedding model\n",
    "try:\n",
    "    hf_embed_model = SentenceTransformer(model, device=model_kwargs['device'])\n",
    "    print(\"Embedding model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {e}\")\n",
    "    \n",
    "#hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "#    model_name=embed_model,\n",
    "#    model_kwargs=model_kwargs,\n",
    "#    encode_kwargs=encode_kwargs\n",
    "#)\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "#load_dotenv()\n",
    "# Initialize OpenAI client with the API key from environment variables\n",
    "#open_ai_embed_model = OpenAIEmbeddings(openai_api_key= os.environ[\"OPENAI_API_KEY\"],\n",
    "#             model=\"text-embedding-3-large\", \n",
    "#             max_retries=1000,\n",
    "#             request_timeout=8,\n",
    "#             retry_min_seconds=4,\n",
    "#             show_progress_bar=True,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestF extended dictionary\n",
    "\n",
    "This HS Code dicitonary was scrapped from 2 different sites to increase redundancy and accuracy.\n",
    "\n",
    "PDFs scrapped from the web - https://www.wcoomd.org/en/topics/nomenclature/instrument-and-tools/hs-nomenclature-2022-edition/hs-nomenclature-2022-edition.aspx\n",
    "\n",
    "Definitions on WebSite - https://www.dripcapital.com/hts-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Pure-bred breeding horses' metadata={'source': '0101.21'}\n",
      "page_content='Live horses (excluding pure-bred for breeding)' metadata={'source': '0101.29'}\n",
      "Amount of documents is:  5541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando embeddings...:   0%|          | 0/5541 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MistralForCausalLM' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     26\u001b[0m     vector_db\u001b[38;5;241m.\u001b[39madd_documents([d])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#When no GPU is available\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m#vector_db = Chroma.from_documents([d],embed_model, persist_directory=persistent_dir )\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m#To enable embeddings running on GPU: embedding and ingesting at the same time\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     vector_db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersistent_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)    \n",
      "File \u001b[1;32mc:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:842\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    837\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    838\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    839\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    840\u001b[0m         )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MistralForCausalLM' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir =  os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir,'..' ,'index', 'gemma-2b-table'))\n",
    "\n",
    "# Importante Leer columna de codigo como string, sino se eliminan los zeros a la izquierda.\n",
    "df = pd.read_csv('..\\data\\WebScrap_CSVs\\hts_codes_WebScrapped.csv', encoding='utf-8', dtype={'HTS code': str})\n",
    "\n",
    "# Split data and metadata\n",
    "texts = df['Description'].tolist()  # This is the text data that will be embedded\n",
    "metadata = df['HTS code'].tolist()  # This is the metadata that will be stored alongside the embeddings\n",
    "\n",
    "documents = []\n",
    "for i, text in enumerate(texts):\n",
    "    document = Document(page_content=text, metadata={\"source\": metadata[i]})\n",
    "    documents.append(document)\n",
    "\n",
    "# Debug to check metadata + text\n",
    "print(documents[0])\n",
    "print(documents[1])    \n",
    "print(\"Amount of documents is: \" , len(documents))   \n",
    "\n",
    "vector_db = None\n",
    "with tqdm(total=len(documents), desc=\"Creando embeddings...\") as pbar:\n",
    "    for d in documents:\n",
    "        if vector_db:\n",
    "            vector_db.add_documents([d])\n",
    "        else:\n",
    "            #When no GPU is available\n",
    "            #vector_db = Chroma.from_documents([d],embed_model, persist_directory=persistent_dir )\n",
    "            \n",
    "            #To enable embeddings running on GPU: embedding and ingesting at the same time\n",
    "            vector_db = Chroma.from_documents([d],model, persist_directory=persistent_dir)\n",
    "        pbar.update(1)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
