{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\Python Projects\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_openai  import OpenAIEmbeddings\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import warnings, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up embedding model to use with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model definition\n",
    "A technique for representing text data as numerical vectors, which can be input into machine learning models. The embedding model is responsible for converting text into these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# WARNING! :Only runs with this version\n",
    "###### !pip install sentence-transformers==2.2.2  ######\n",
    "#Define the sentence-transformer model:\n",
    "\n",
    "#For English\n",
    "#sentence-transformers/LaBSE\n",
    "#embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#embed_model = \"BAAI/bge-multilingual-gemma2\"\n",
    "#embed_model = \"dunzhang/stella_en_1.5B_v5\"\n",
    "\n",
    "#For Spanish \n",
    "#projecte-aina/aguila-7b\n",
    "#embed_model = \"hiiamsid/sentence_similarity_spanish_es\"\n",
    "\n",
    "#Other sentence-transformer settings\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=embed_model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "#load_dotenv()\n",
    "# Initialize OpenAI client with the API key from environment variables\n",
    "#open_ai_embed_model = OpenAIEmbeddings(openai_api_key= os.environ[\"OPENAI_API_KEY\"],\n",
    "#             model=\"text-embedding-3-large\", \n",
    "#             max_retries=1000,\n",
    "#             request_timeout=8,\n",
    "#             retry_min_seconds=4,\n",
    "#             show_progress_bar=True,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestF extended dictionary\n",
    "\n",
    "This HS Code dicitonary was scrapped from 2 different sites to increase redundancy and accuracy.\n",
    "\n",
    "PDFs scrapped from the web - https://www.wcoomd.org/en/topics/nomenclature/instrument-and-tools/hs-nomenclature-2022-edition/hs-nomenclature-2022-edition.aspx\n",
    "\n",
    "Definitions on WebSite - https://www.dripcapital.com/hts-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando embeddings...:  57%|█████▋    | 16250/28305 [06:09<04:28, 44.88it/s]"
     ]
    }
   ],
   "source": [
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir = os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir, '..', 'index', 'TestF'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert text to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove non-alphanumeric characters (keep spaces)\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    # Lemmatization to convert the received sentence to a meaningful sentence\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join words back into a sentence\n",
    "    meaningful_text = ' '.join(words)\n",
    "    return meaningful_text \n",
    "\n",
    "# Function to extract metadata (HS code) and content\n",
    "def extract_metadata_and_content(line):\n",
    "    #match = re.match(r\"(\\d{4}\\.\\d{2})\\s*(.*)\", line)\n",
    "    \n",
    "    #Search for any number with 2 decimals like 01.01\n",
    "    #Bypass chaptpers\n",
    "    #match = re.match(r\"(\\d+\\.\\d+)\\s*(.*)\", line)\n",
    "    match = re.match(r\"(\\d+\\.\\d+(?:\\.\\d{2})?)\\s*(.*)\", line)\n",
    "    if match:\n",
    "        metadata = match.group(1)  # The number part as metadata\n",
    "        content = match.group(2)   # The rest of the line as content\n",
    "        content = clean_text(content)\n",
    "        return metadata, content\n",
    "    return None, line  # In case no match, return the line as-is\n",
    "\n",
    "\n",
    "#with open('..\\data\\hs_code_dictionary_extended.txt', 'r', encoding='utf-8') as f1, open('..\\data\\hs_code_dictionary.txt', 'r', encoding='utf-8') as f2, open('..\\data\\merged.txt', 'w', encoding='utf-8') as mf:\n",
    "#    # Read contents from both files\n",
    "#    file1_content = f1.read()\n",
    "#    file2_content = f2.read()\n",
    "#    \n",
    "#    # Write contents to the new file\n",
    "#    mf.write(file1_content)\n",
    "#    mf.write(\"\\n\")  # Add a newline between the contents of the two files if needed\n",
    "#    mf.write(file2_content)\n",
    "\n",
    "# Step 1: Read the .txt file\n",
    "#file_path = '..\\data\\hs_code_dictionary_extended.txt'  # Path to your .txt file\n",
    "with open('..\\data\\merged.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Split the text into lines first\n",
    "lines = text.split(\"\\n\")\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "chunk_size = 150  # Define your chunk size\n",
    "chunk_overlap = 0  # Set to 0 if you don't want overlapping chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Step 3: Process each line, extract metadata, split content, and create Document objects\n",
    "documents = []\n",
    "for line in lines:\n",
    "    metadata, content = extract_metadata_and_content(line)\n",
    "    \n",
    "    if metadata:  # If we found valid metadata\n",
    "        # Split the content into chunks\n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        # Create a Document object for each chunk, with metadata containing the HS code\n",
    "        # Si el texto en la descripcion es demasiado largo, lo cortamos en chunks para obtener\n",
    "        # palabras clave pero con el contexto suficiente\n",
    "        for chunk in chunks:\n",
    "            document = Document(page_content=chunk, metadata={\"source\": metadata})\n",
    "            #print(metadata, chunk)\n",
    "            documents.append(document)\n",
    "\n",
    "# Step 4: Embed the documents into the vector database\n",
    "vector_db = None\n",
    "with tqdm(total=len(documents), desc=\"Creando embeddings...\") as pbar:\n",
    "    for d in documents:\n",
    "        if vector_db:\n",
    "            vector_db.add_documents([d])\n",
    "        else:\n",
    "            # When no GPU is available, initialize vector_db\n",
    "            #vector_db = Chroma.from_documents([d], embed_model, persist_directory=persistent_dir)\n",
    "            \n",
    "            # To enable embeddings running on GPU, ingest documents and create embeddings\n",
    "            vector_db = Chroma.from_documents([d], hf_embed_model, persist_directory=persistent_dir)\n",
    "        pbar.update(1)\n",
    "        #print([d])\n",
    "\n",
    "# The 'documents' list now contains Document objects with metadata and chunks, indexed into the vector database\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
