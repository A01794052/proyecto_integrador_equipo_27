{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\maestria\\Proyecto Integrador\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "#import dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import warnings\n",
    "from rich import print\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "# Define the persistent directory containing the VectorDB\n",
    "script_dir =  os.getcwd()\n",
    "persistent_dir = os.path.abspath(os.path.join(script_dir,'..' ,'index', 'WebScrap_CSVs'))\n",
    "\n",
    "# SE REQUIERE UTILIZAR EL MISMO MODELO DE SENTENCE-TRANSFORMER CON EL QUE SE CREO EL INDEX, sin ello se enfrentaran:\n",
    "#Espacios Vectoriales Diferentes:\n",
    "#\n",
    "#Cada modelo de sentence-transformer genera embeddings en su propio espacio vectorial. Estos espacios son definidos por los pesos y arquitecturas específicas de cada modelo.\n",
    "#Los embeddings generados por el modelo A no son directamente comparables con los del modelo B porque están en espacios diferentes y no alineados.\n",
    "#Incompatibilidad de Embeddings:\n",
    "#\n",
    "#Las representaciones vectoriales (embeddings) de las mismas frases pueden ser muy distintas entre modelos. Por ejemplo, una oración podría tener un vector [0.1, 0.2, 0.3] en el modelo A y [0.5, -0.1, 0.2] en el modelo B.\n",
    "#Esto significa que medidas de similitud como la similitud coseno no serán significativas, ya que los vectores no están en el mismo espacio.\n",
    "#Resultados Incorrectos o Sin Sentido:\n",
    "#\n",
    "#Al realizar consultas, las comparaciones entre embeddings del índice (modelo A) y los embeddings de la consulta (modelo B) producirán resultados erróneos.\n",
    "#Podrías obtener altas similitudes entre oraciones no relacionadas o bajas similitudes entre oraciones muy similares.\n",
    "#Pérdida de Precisión y Rendimiento:\n",
    "#\n",
    "#El rendimiento del sistema de recuperación de información se degradará significativamente.\n",
    "#Los usuarios recibirán resultados irrelevantes, lo que afecta la usabilidad y confiabilidad del sistema.\n",
    "#Consistencia en Procesamiento de Lenguaje Natural:\n",
    "#\n",
    "#Los modelos pueden tener diferentes enfoques para manejar ciertos aspectos del lenguaje, como negaciones, sarcasmo o lenguaje coloquial.\n",
    "#Esto añade otra capa de inconsistencia entre los embeddings generados por diferentes modelos.\n",
    "\n",
    "embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "# Para correr las predicciones en GPU\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A Chain\n",
    "\n",
    "This will run peristantly, consulting already stored vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a LLM Chain to provide context and system prompts on every query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">The requested item's description to search HTS code is:</span>\n",
       "A Live Dog of breed Schnauzer from Germany\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mThe requested item's description to search HTS code is:\u001b[0m\n",
       "A Live Dog of breed Schnauzer from Germany\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">The response of the LLM is:</span>\n",
       "Your inquiry is very subjective, to provide an accurate HS Code I need you to be more specific, please include \n",
       "size, weight, origin <span style=\"font-weight: bold\">(</span>Germany<span style=\"font-weight: bold\">)</span>, meaning of use, etc.\n",
       "\n",
       "So far, I can narrow it to this chapters/parts : \n",
       "\n",
       "* Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>: Live animals\n",
       "* Subchapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0101</span>: Horses and other equines\n",
       "* Subchapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0102</span>: Bovine animals\n",
       "* Subchapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0103</span>: Swine\n",
       "* Subchapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0105</span>: Mammals, alive, n.e.s.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mThe response of the LLM is:\u001b[0m\n",
       "Your inquiry is very subjective, to provide an accurate HS Code I need you to be more specific, please include \n",
       "size, weight, origin \u001b[1m(\u001b[0mGermany\u001b[1m)\u001b[0m, meaning of use, etc.\n",
       "\n",
       "So far, I can narrow it to this chapters/parts : \n",
       "\n",
       "* Chapter \u001b[1;36m01\u001b[0m: Live animals\n",
       "* Subchapter \u001b[1;36m0101\u001b[0m: Horses and other equines\n",
       "* Subchapter \u001b[1;36m0102\u001b[0m: Bovine animals\n",
       "* Subchapter \u001b[1;36m0103\u001b[0m: Swine\n",
       "* Subchapter \u001b[1;36m0105\u001b[0m: Mammals, alive, n.e.s.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "##### Embedding model (sentence-transformer)\n",
    "model_name = embed_model\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "##### LLM model\n",
    "# loading the Llama3 model from local\n",
    "llm_model = OllamaLLM(model=\"llama3.1\",\n",
    "                temperature=0.7,\n",
    "                num_thread=8,\n",
    "                )\n",
    "# loading the vectorstore\n",
    "vectorstore = Chroma(persist_directory=persistent_dir, embedding_function=hf_embed_model)\n",
    "# casting  the vectorstore as the retriever, taking the best 3 similarities\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":5})\n",
    "\n",
    "# formating the docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "template = \"\"\"Always answer in English. \n",
    "If you can infer an specific code (based on a very good match) you must respond only with this kind fo format \"XXXX.XX\" and answer it at the beggining with: \n",
    "\"The suggested HS code is: \" \n",
    "\n",
    "In the case you can't find a matching code for the description, respond with:\n",
    "'Your inquiry is very subjective, to provide an accurate HS Code I need you to be more specific, please include size, weight, origin, meaning of use, etc. \n",
    "So far, I can narrow it to this chapters/parts : 'provide 3 possible codes\".\n",
    "\n",
    "context:\n",
    "{summaries}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Define the LLM chain (using the Llama3.1 model)\n",
    "llm_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm_model,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # To get both the answer and source docs\n",
    "    chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                #For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "                input_variables=[\"question\", \"summaries\"],\n",
    "            ),\n",
    "        },\n",
    ")\n",
    "\n",
    "context = \"\"\"As a logistics shipping arrival inspector, your primary responsibility is to inspect incoming shipments and accurately classify goods \n",
    "using the Harmonized System (HS) code based on the descriptions provided in the shipping manifests. You will thoroughly review the manifest details, \n",
    "including product type, material composition, function, and intended use, to determine the correct HS code. \n",
    "\n",
    "Your task is to:\n",
    "Carefully read and analyze the product descriptions from the manifest.\n",
    "Identify key characteristics of the goods, such as \n",
    "type (e.g., electronics, textiles, machinery), \n",
    "material (e.g., plastic, metal, organic), \n",
    "and usage (e.g., household, industrial, medical).\n",
    "Use your knowledge of the HS code classification system to assign the most appropriate HS code for each product based on its description.\n",
    "Ensure compliance with international trade regulations by selecting precise codes to avoid delays or penalties.\n",
    "Remember to be thorough and accurate in your classification, as this impacts customs processing, tariffs, and legal requirements.\"\"\"\n",
    "\n",
    "\n",
    "#-----Benchmark query------\n",
    "#Response should be: \"0106.19\" \n",
    "#query = \"Live Dog\"\n",
    "query = \"A Live Dog of breed Schnauzer from Germany\"\n",
    "#query = \"Live Pig\"\n",
    "#query = \"Live Buffalo\"\n",
    "#query = \"What HS Code belongs to a wrench?\"\n",
    "#query = \"What HS Code belongs to a live dog?\"\n",
    "#query = \"What HS Code belongs to:'CERAMIC TABLEWARE  2688 BOXES PO 5 39548'?\"\n",
    "#query = \"Covers for cellphone screen\"\n",
    "\n",
    "# Execute the chain with the query\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "result = llm_chain({\"question\": query, \"summaries\": context})\n",
    "\n",
    "# Process the results\n",
    "#print(result.keys())\n",
    "print(f\"[bold yellow]The requested item's description to search HTS code is:[/bold yellow]\\n{query}\")\n",
    "print(f\"[bold green]The response of the LLM is:[/bold green]\\n{result['answer']}\")\n",
    "\n",
    "#print(\"The documents content used for this response are:\")\n",
    "#for i in range(len(result[\"source_documents\"])):\n",
    "#    print(result[\"source_documents\"][i].page_content)\n",
    "#    print(result[\"source_documents\"][i].metadata)\n",
    "     \n",
    "#results = vectorstore.similarity_search_with_score(query=query, k=3)\n",
    "## Print similarity results\n",
    "#for doc, score in results:\n",
    "#    print(f\"Document content: {doc.page_content}, Code: {doc.metadata},Similarity Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print it on a nicer format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
