{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install openpyxl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install transformers sentencepiece\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install transformers==4.30.0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install torch\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      HS Code                                   Full description\n","0          01                           Chapter 1   Live animals\n","1       01.01             Live horses, asses, mules and hinnies.\n","2  0101.21.00  Live horses, asses, mules and hinnies. - > Hor...\n","3  0101.29.00  Live horses, asses, mules and hinnies. - > Hor...\n","4     0101.30   Live horses, asses, mules and hinnies. -> Asses:\n","                                      Raw_data_input  \\\n","0  CERAMIC TABLEWARE HS CODE 691200 2688 BOXES PO...   \n","1  1X40HR SAID TO CONTAIN 13 PACKAGE SUDU5157624 ...   \n","2  KONTRAKT  QHBG038545 LEERCONTAINER EX CDN NURN...   \n","3               MUEBLES Y ESTANTERIAS HS CODE 940310   \n","4  CHEMICAL PRODUCTS NOT SUBJECT TO IMDG CODEIMBE...   \n","5  1X20DC. SAID TO CONTAIN 21 PKGS IRON STEEL IRO...   \n","6  BITUMINOUS WATERPROOFING  MEMBRANEPRODUCT NAME...   \n","7            AUTOMOTIVE SPARE PARTS HS CODE 87085099   \n","8  NET WEIGHT 21500 KG; GROSS VOLUME 68 M3 GIRASO...   \n","9  2 PACAKGESBOS AUTOMOTIVE360 KGMPRODUCTS1 056 M...   \n","\n","                                    Validation_input  Expected_output  \\\n","0           CERAMIC TABLEWARE  2688 BOXES PO 5 39548           691200   \n","1  1X40HR SAID TO CONTAIN 13 PACKAGE SUDU5157624 ...         21069098   \n","2  KONTRAKT  QHBG038545 LEERCONTAINER EX CDN NURN...           870899   \n","3                             MUEBLES Y ESTANTERIAS            940310   \n","4  CHEMICAL PRODUCTS NOT SUBJECT TO IMDG CODEIMBE...           340213   \n","5  1X20DC. SAID TO CONTAIN 21 PKGS IRON STEEL IRO...         72222039   \n","6  BITUMINOUS WATERPROOFING  MEMBRANEPRODUCT NAME...         68071000   \n","7                            AUTOMOTIVE SPARE PARTS          87085099   \n","8  NET WEIGHT 21500 KG; GROSS VOLUME 68 M3 GIRASO...             1206   \n","9  2 PACAKGESBOS AUTOMOTIVE360 KGMPRODUCTS1 056 M...           760421   \n","\n","   Expected_output_two_digits  Expected_output_four_digits  \n","0                          69                         6912  \n","1                          21                         2106  \n","2                          87                         8708  \n","3                          94                         9403  \n","4                          34                         3402  \n","5                          72                         7222  \n","6                          68                         6807  \n","7                          87                         8708  \n","8                          12                         1206  \n","9                          76                         7604  \n"]}],"source":["import pandas as pd\n","import re\n","import string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","\n","# Define the paths to your CSV files\n","hs_dictionary_path = r\"C:\\Users\\juan-\\Documents\\Proyecto Hacku AI\\Repositorio GitHub página\\proyecto_integrador_equipo_27\\Avance_2\\HS code Catalogue_Processed.csv\"\n","Validation_input_path = r\"C:\\Users\\juan-\\Documents\\Proyecto Hacku AI\\Repositorio GitHub página\\proyecto_integrador_equipo_27\\Avance_2\\Validation Data.csv\"\n","\n","# Read the CSV files into DataFrames, specifying the encoding\n","try:\n","    hs_dictionary_df = pd.read_csv(hs_dictionary_path, encoding='utf-8')\n","except UnicodeDecodeError:\n","    hs_dictionary_df = pd.read_csv(hs_dictionary_path, encoding='latin1')\n","\n","try:\n","    Validation_input_df = pd.read_csv(Validation_input_path, encoding='utf-8')\n","except UnicodeDecodeError:\n","    Validation_input_df = pd.read_csv(Validation_input_path, encoding='latin1')\n","\n","# Print the first few rows to ensure the content is read correctly\n","print(hs_dictionary_df.head())\n","print(Validation_input_df.head(10))\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\juan-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package words to\n","[nltk_data]     C:\\Users\\juan-\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\juan-\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\juan-\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\juan-\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","C:\\Users\\juan-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","C:\\Users\\juan-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n","C:\\Users\\juan-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(checkpoint_file, map_location=\"cpu\")\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","C:\\Users\\juan-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3745: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n","\n","If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n","this:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","labels = tokenizer(text_target=tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n","C:\\Users\\juan-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Processed Text: live dog eating bulldog breed\n","Top Tokens: ['bulldog', 'live', 'breed']\n","Processed Text: ceramic tableware\n","Top Tokens: ['ceramic', 'ware', 'table']\n"]}],"source":["import pandas as pd\n","from transformers import MarianMTModel, MarianTokenizer, BertTokenizer, BertModel\n","import nltk\n","from nltk.corpus import words, stopwords\n","from nltk.stem import WordNetLemmatizer\n","import string\n","import re\n","import torch\n","from collections import Counter\n","\n","# Download necessary NLTK data\n","nltk.download('words')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Load the translation model (use Spanish-to-English model)\n","model_name = 'Helsinki-NLP/opus-mt-es-en'  # Spanish to English model\n","tokenizer = MarianTokenizer.from_pretrained(model_name)\n","model = MarianMTModel.from_pretrained(model_name)\n","\n","# Load BERT model for attention scoring\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Get the English words corpus and Spanish stopwords\n","english_words = set(words.words())\n","spanish_stopwords = set(stopwords.words('spanish'))\n","english_stopwords = set(stopwords.words('english'))\n","\n","# Add your custom stopwords to the Spanish stopwords\n","additional_stopwords = {\n","    'abv', 'accessories', 'aduana', 'agent', 'ancho', 'approximately', 'approx', 'ata', 'atd', 'attn', 'attention',\n","    'awb', 'b', 'bag', 'bags', 'barge', 'batch', 'big', 'bill', 'bl', 'bol', 'bonded', 'bottle', 'bottles',\n","    'box', 'boxes', 'broker', 'bulk', 'bundle', 'bundles', 'cargo', 'carrier', 'carton', 'cartons', 'case', 'cases',\n","    'cb', 'cbm', 'celsius', 'center', 'certificate', 'certification', 'cfr', 'charge', 'charges', 'cif', 'cip', 'circa',\n","    'claim', 'class', 'classification', 'clearance', 'cl', 'closing', 'cm', 'cmr', 'cntr', 'code', 'codes', 'collect',\n","    'collection', 'commercial', 'commodity', 'consolidated', 'consolidator', 'consignee', 'consignor', 'contain',\n","    'container', 'containers', 'containing', 'contains', 'contact', 'coo', 'cross', 'ctr', 'ctpat', 'cube', 'cubic',\n","    'customs', 'cut-off', 'damage', 'dangerous', 'dap', 'date', 'dated', 'dc', 'ddp', 'deadline', 'declared',\n","    'declares', 'declaration', 'degr', 'degrees', 'delay', 'delivery', 'demurrage', 'description', 'destination',\n","    'detention', 'dg', 'diameter', 'dim', 'dimension', 'dimensions', 'discharge', 'distribution', 'dock',\n","    'documentation', 'dt', 'dtd', 'duty', 'each', 'eccn', 'email', 'entry', 'eta', 'etd', 'event', 'exception',\n","    'excise', 'export', 'exportation', 'exw', 'fas', 'fax', 'fcl', 'feeder', 'fees', 'feu', 'flat', 'flight', 'fob',\n","    'forwarder', 'free', 'freight', 'gp', 'gross', 'gst', 'handling', 'hazardous', 'hbl', 'hc', 'height', 'high',\n","    'hr', 'hs', 'hs-code', 'hscode', 'hts', 'id', 'imdg', 'imo', 'import', 'importation', 'impression', 'impresion',\n","    'incoterms', 'inch', 'inches', 'inspection', 'insurance', 'intermodal', 'inv', 'invoice', 'invoices', 'iso',\n","    'item', 'items', 'iva', 'jumbo', 'kg', 'kgs', 'lading', 'largo', 'lcl', 'length', 'license', 'load', 'loaded',\n","    'loading', 'logistics', 'loose', 'loss', 'lot', 'lote', 'm3', 'manifest', 'marks', 'mbl', 'measurement',\n","    'measurements', 'medium', 'meter', 'metric', 'milestone', 'mm', 'mode', 'mt', 'multimodal', 'ncm', 'net',\n","    'netweight', 'no', 'non-hazardous', 'non-stackable', 'note', 'notes', 'notify', 'number', 'numbers', 'nvocc',\n","    'of', 'only', 'open', 'order', 'origin', 'packaging', 'packing', 'pallet', 'pallets', 'palletized', 'party',\n","    'payable', 'pc', 'pcs', 'permit', 'phytosanitary', 'pickup', 'piece', 'pieces', 'pkg', 'pkgs', 'pl', 'plt',\n","    'plts', 'po', 'port', 'prepaid', 'qty', 'quantity', 'quota', 'rack', 'rail', 'reefer', 'reel', 'reels', 'ref',\n","    'reference', 'report', 'restriction', 'rfc', 'roll', 'rolls', 'routing', 'sack', 'sacks', 'said', 'sailing',\n","    'scac', 'schedule', 'seal', 'seals', 'serial', 'sheets', 'shipment', 'shipper', 'shipping', 'sin', 'size',\n","    'sizes', 'small', 'stackable', 'status', 'stc', 'storage', 'stowed', 'stuffing', 'surcharges', 'survey',\n","    'system', 'tank', 'tare', 'tariff', 'tax', 'temperature', 'temp', 'terminal', 'teu', 'time', 'to', 'ton',\n","    'tons', 'top', 'total', 'tracking', 'tracing', 'trade', 'transit', 'transload', 'transhipment', 'transport',\n","    'truck', 'un', 'unit', 'unstuffing', 'valuation', 'value', 'vat', 'vessel', 'volume', 'voyage', 'warehouse',\n","    'weight', 'width', 'x', 'zone'\n","}\n","\n","\n","stop_words = spanish_stopwords.union(additional_stopwords)\n","stop_words = stop_words.union(english_stopwords)\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","    return text.strip()\n","\n","# Function to translate text to English\n","def translate_to_english(text, tokenizer, model):\n","    translated = model.generate(**tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\"))\n","    return tokenizer.decode(translated[0], skip_special_tokens=True)\n","\n","# Function to lemmatize, remove nonsense words, and clean text\n","def lemmatize_and_clean(text):\n","    words = text.split()\n","    cleaned_words = []\n","    for word in words:\n","        lemma = lemmatizer.lemmatize(word)\n","        if lemma in english_words and lemma not in stop_words:\n","            cleaned_words.append(lemma)\n","    return cleaned_words  # Return list of cleaned words\n","\n","# Function to get top tokens based on frequency and attention scores\n","def get_top_tokens(text, top_n=3):\n","    inputs = bert_tokenizer(text, return_tensors='pt')\n","    outputs = bert_model(**inputs)\n","    \n","    # Get the attention scores (mean pooling over token embeddings)\n","    attention_scores = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n","    \n","    # Create a dictionary of tokens and their corresponding attention scores\n","    tokens_with_scores = {token_id: score for token_id, score in zip(inputs['input_ids'][0], attention_scores)}\n","    \n","    # Filter out special tokens and get full tokens without subword prefixes\n","    filtered_tokens_scores = {}\n","    \n","    for token_id, score in tokens_with_scores.items():\n","        if token_id not in [bert_tokenizer.pad_token_id, bert_tokenizer.cls_token_id, bert_tokenizer.sep_token_id]:\n","            token_str = bert_tokenizer.decode([token_id]).replace(\"##\", \"\")\n","            filtered_tokens_scores[token_str] = score\n","    \n","    # Count occurrences of each token in the cleaned words list for additional scoring\n","    word_counts = Counter(lemmatize_and_clean(text))\n","    \n","    # Combine frequency counts with attention scores for final ranking\n","    combined_scores = {token: word_counts[token] + filtered_tokens_scores.get(token, 0) for token in filtered_tokens_scores}\n","    \n","    # Sort by combined score and get top N tokens\n","    top_tokens = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)[:top_n]\n","    \n","    return [token for token, _ in top_tokens]\n","\n","# Full pipeline function\n","def process_text(text):\n","    try:\n","        # Step 1: Preprocess\n","        preprocessed_text = preprocess_text(text)\n","        \n","        # Step 2: Translate\n","        translated_text = translate_to_english(preprocessed_text, tokenizer, model)\n","        \n","        # Step 3: Lemmatize, remove nonsense words, and clean\n","        cleaned_words_list = lemmatize_and_clean(translated_text)\n","        \n","        # Convert cleaned words list back to string for attention scoring\n","        final_cleaned_text = \" \".join(cleaned_words_list)\n","\n","        # Step 4: Get top tokens based on frequency and attention scores\n","        top_tokens = get_top_tokens(final_cleaned_text)\n","\n","        return final_cleaned_text, top_tokens\n","\n","    except Exception as e:\n","        print(f\"Error processing text: {text}\")\n","        print(f\"Error message: {str(e)}\")\n","        return \"\", []  # Return empty string and empty list in case of error\n","\n","# Example usage:\n","if __name__ == \"__main__\":\n","    sample_text_1 = \"Perro vivo que come dogchow raza bulldog\"\n","    processed_text_1, important_tokens_1 = process_text(sample_text_1)\n","    \n","    print(\"Processed Text:\", processed_text_1)\n","    print(\"Top Tokens:\", important_tokens_1)\n","\n","    sample_text_2 = \"CERAMIC TABLEWARE 2688 BOXES PO 5 39548\"\n","    processed_text_2, important_tokens_2 = process_text(sample_text_2)\n","\n","    print(\"Processed Text:\", processed_text_2)\n","    print(\"Top Tokens:\", important_tokens_2)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Apply the full pipeline to the 'Validation_input' column\n","validation_cleaned_input = Validation_input_df.copy()\n","validation_cleaned_input['Validation_input_cleaned'] = Validation_input_df['Validation_input'].dropna().apply(process_text)\n","\n","# Export the cleaned DataFrame to an Excel file\n","output_file = 'cleaned_validation_input.xlsx'\n","validation_cleaned_input.to_excel(output_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["process_text('NEW PNEUMATIC RADIAL TIREs 10')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the English words corpus and Spanish stopwords\n","english_words = set(words.words())\n","spanish_stopwords = set(stopwords.words('spanish'))\n","\n","# Convert both to sorted lists for easier manipulation\n","english_words_list = sorted(list(english_words))\n","spanish_stopwords_list = sorted(list(spanish_stopwords))\n","\n","# Create a pandas DataFrame from both lists\n","df = pd.DataFrame({\n","    'English_Words': pd.Series(english_words_list),\n","    'Spanish_Stopwords': pd.Series(spanish_stopwords_list)\n","})\n","\n","# Save the DataFrame to an Excel file\n","output_file = 'words_and_stopwords.xlsx'\n","df.to_excel(output_file, index=False)\n","\n","print(f\"Data has been saved to {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check downloaded words and stopwords\n","print(f\"Number of English words: {len(english_words)}\")\n","print(f\"Number of Spanish stopwords: {len(spanish_stopwords)}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
