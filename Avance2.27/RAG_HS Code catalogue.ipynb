{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\maestria\\Proyecto Integrador\\proyecto_integrador_equipo_27\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "#import dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "## To run Hugging Face OpenSource models\n",
    "# Needs to manually install Visual C++ Tools from: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device Name: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Chunking\n",
    "Is the process of breaking down a large input text into smaller pieces.\n",
    "This ensures that the text fits the input size of the embedding model and improves retrieval efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.core import SimpleDirectoryReader\n",
    "#WARNING!: When using this, any \"page_content\" furthewr instructon won't work.\n",
    "## load data\n",
    "#loader = SimpleDirectoryReader(\n",
    "#            input_dir = 'Demanda',\n",
    "#            required_exts=[\".pdf\"],\n",
    "#            recursive=True\n",
    "#        )\n",
    "#docs = loader.load_data()\n",
    "\n",
    "\n",
    "#loader = PyPDFLoader(r\"Casa\\Codigo_Civil_del_Estado_de_San_Luis_Potosi_16_May_2024_compressed.pdf\")\n",
    "#documents = loader.load()\n",
    "#print(len(documents))\n",
    "\n",
    "# Define the directory containing the PDF files\n",
    "# Step 1: Load your CSV file\n",
    "df = pd.read_csv('HS Code catalogue.csv')\n",
    "\n",
    "# Split data and metadata\n",
    "texts = df['description'].tolist()  # This is the text data that will be embedded\n",
    "metadata = df['code'].tolist()  # This is the metadata that will be stored alongside the embeddings\n",
    "\n",
    "\n",
    "# Define the persistent directory containing the VectorDB\n",
    "persistent_dir= \"VectorDB/HS_codes/model_with_HS_Code_catalogue\"\n",
    "\n",
    "# WARNING! :Only runs with this version\n",
    "###### !pip install sentence-transformers==2.2.2  ######\n",
    "#Define the sentence-transformer model:\n",
    "\n",
    "#For English\n",
    "#sentence-transformers/LaBSE\n",
    "#embed_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "#For Spanish \n",
    "#projecte-aina/aguila-7b\n",
    "#embed_model = \"hiiamsid/sentence_similarity_spanish_es\"\n",
    "\n",
    "#Other sentence-transformer settings\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- Embedding\n",
    "\n",
    "A technique for representing text data as numerical vectors, which can be input into machine learning models. The embedding model is responsible for converting text into these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "#Only runs with this version\n",
    "#!pip install sentence-transformers==2.2.2\n",
    "\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=embed_model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.- Vector DB\n",
    "\n",
    "A collection of pre-computed vector representations of text data for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, and horizontal scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Horses: live, pure-bred breeding animals' metadata={'source': 10121}\n",
      "page_content='Horses: live, other than pure-bred breeding animals' metadata={'source': 10129}\n",
      "page_content='Asses: live' metadata={'source': 10130}\n"
     ]
    }
   ],
   "source": [
    "chroma_db = Chroma(\n",
    "    collection_name=\"csv_collection\",  # Name for the Chroma collection\n",
    "    embedding_function=hf_embed_model.embed_query,  # Function for query embeddings\n",
    "    persist_directory=persistent_dir\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for i, text in enumerate(texts):\n",
    "    document = Document(page_content=text, metadata={\"source\": metadata[i]})\n",
    "    documents.append(document)\n",
    "\n",
    "# Debug to check metadata + text\n",
    "print(documents[0])\n",
    "print(documents[1])    \n",
    "print(documents[2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings...: 100%|██████████| 5384/5384 [02:03<00:00, 43.50it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_db = None\n",
    "with tqdm(total=len(documents), desc=\"Creating embeddings...\") as pbar:\n",
    "    for d in documents:\n",
    "        if vector_db:\n",
    "            vector_db.add_documents([d])\n",
    "        else:\n",
    "            #When no GPU is available\n",
    "            #vector_db = Chroma.from_documents([d],embed_model, persist_directory=persistent_dir )\n",
    "            \n",
    "            #To enable embeddings running on GPU: embedding and ingesting at the same time\n",
    "            vector_db = Chroma.from_documents([d],hf_embed_model, persist_directory=persistent_dir)\n",
    "        pbar.update(1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple test over Vector DB\n",
    "\n",
    "This response would contain the answer to that question, this needs to be sent to the LLM to paraphrase and extract only the needed info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 10391}, page_content='Swine: live, other than pure-bred breeding animals, weighing less than 50kg'), Document(metadata={'source': 10392}, page_content='Swine: live, other than pure-bred breeding animals, weighing 50kg or more'), Document(metadata={'source': 10310}, page_content='Swine: live, pure-bred breeding animals'), Document(metadata={'source': 10410}, page_content='Sheep: live')]\n"
     ]
    }
   ],
   "source": [
    "question = \"Live Pig? \"\n",
    "docs = vector_db.similarity_search(question)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A Chain\n",
    "\n",
    "This will run peristantly, consulting already stored vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a LLM Chain to provide context and system prompts on every query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "The response of the LLM is:  The suggested HS code is: 0109.10\n"
     ]
    }
   ],
   "source": [
    "#from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "##### Embedding model (sentence-transformer)\n",
    "model_name = embed_model\n",
    "model_kwargs = {'device': 'cuda:0'}  # specify GPU device\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf_embed_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "##### LLM model\n",
    "# loading the Llama3 model from local\n",
    "llm_model = OllamaLLM(model=\"llama3.1\",\n",
    "                temperature=0.1,\n",
    "                num_thread=8,\n",
    "                )\n",
    "# loading the vectorstore\n",
    "vectorstore = Chroma(persist_directory=persistent_dir, embedding_function=hf_embed_model)\n",
    "# casting  the vectorstore as the retriever, taking the best 3 similarities\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "# formating the docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "template = \"\"\"Always answer in English. \n",
    "If you can infer an specific code (based on a decent match) you must respond only with this kind fo format \"XXXX.XX\" and answer it at the beggining with: \n",
    "\"The suggested HS code is: \" \n",
    "\n",
    "In the case you can't find a matching code for the description, respond with:\n",
    "'Your inquiry is very subjective, to provide an accurate HS Code I need you to be more specific, please include size, weight, origin, meaning of use, etc. \n",
    "So far, I can narrow it to this chapters/parts : 'provide 3 possible codes\".\n",
    "\n",
    "context:\n",
    "{summaries}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Define the LLM chain (using the Llama3.1 model)\n",
    "llm_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm_model,\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # To get both the answer and source docs\n",
    "    chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                #For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "                input_variables=[\"question\", \"summaries\"],\n",
    "            ),\n",
    "        },\n",
    ")\n",
    "\n",
    "context = \"\"\"As a logistics shipping arrival inspector, your primary responsibility is to inspect incoming shipments and accurately classify goods \n",
    "using the Harmonized System (HS) code based on the descriptions provided in the shipping manifests. You will thoroughly review the manifest details, \n",
    "including product type, material composition, function, and intended use, to determine the correct HS code. \n",
    "\n",
    "Your task is to:\n",
    "Carefully read and analyze the product descriptions from the manifest.\n",
    "Identify key characteristics of the goods, such as \n",
    "type (e.g., electronics, textiles, machinery), \n",
    "material (e.g., plastic, metal, organic), \n",
    "and usage (e.g., household, industrial, medical).\n",
    "Use your knowledge of the HS code classification system to assign the most appropriate HS code for each product based on its description.\n",
    "Ensure compliance with international trade regulations by selecting precise codes to avoid delays or penalties.\n",
    "Remember to be thorough and accurate in your classification, as this impacts customs processing, tariffs, and legal requirements.\"\"\"\n",
    "\n",
    "\n",
    "#-----Benchmark query------\n",
    "#Response should be: \"0106.19\" \n",
    "#query = \"Live Dog\"\n",
    "query = \"A Live Dog of breed Schnauzer from Germany\"\n",
    "#query = \"Live Pig\"\n",
    "#query = \"Live Buffalo\"\n",
    "#query = \"What HS Code belongs to a wrench?\"\n",
    "#query = \"What HS Code belongs to a live dog?\"\n",
    "#query = \"What HS Code belongs to:'CERAMIC TABLEWARE  2688 BOXES PO 5 39548'?\"\n",
    "#query = \"Covers for cellphone screen\"\n",
    "\n",
    "# Execute the chain with the query\n",
    "#For some reason, \"context\" cant be used as input variable, it should be named as \"summaries\"\n",
    "result = llm_chain({\"question\": query, \"summaries\": context})\n",
    "\n",
    "# Process the results\n",
    "#print(result.keys())\n",
    "print(\"The response of the LLM is: \", result[\"answer\"])\n",
    "\n",
    "#print(\"The documents content used for this response are:\")\n",
    "#for i in range(len(result[\"source_documents\"])):\n",
    "#    print(result[\"source_documents\"][i].page_content)\n",
    "#    print(result[\"source_documents\"][i].metadata)\n",
    "     \n",
    "#results = vectorstore.similarity_search_with_score(query=query, k=3)\n",
    "## Print similarity results\n",
    "#for doc, score in results:\n",
    "#    print(f\"Document content: {doc.page_content}, Code: {doc.metadata},Similarity Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
